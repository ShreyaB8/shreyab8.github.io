---
layout: about
title: About
permalink: /
subtitle: An <a href='#'>AI Hardware</a> Blog and other things I want to ramble about

profile:
  align: right
  image: prof_pic.jpg
  image_circular: True # crops the image to make it circular
  more_info: >
    <p>ğŸ“ Austin, Texas, USA</p>
    <p>ğŸ¤ Consult and Collab <a href="https://calendly.com/shreyabw834/30min?month=2025-01">Calendly</a></p>
    <p>ğŸ“ View my <a href="/assets/pdf/Shreya_CV.pdf" target="_blank">Resume</a></p>
    <p>âœ‰ï¸ shreyashri.biswas@amd.com</p>
    <div style="display: flex; justify-content: center; gap: 10px; margin-top: 10px;">
        <img src="/assets/img/1.png" alt="AMD" style="width: 50px; height: 50px;">
        <img src="/assets/img/2.png" alt="CMU" style="width: 50px; height: 50px;">
    </div>

# news: false # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

Passionate about accelerating LLM inference with cutting-edge techniques like speculative decoding and kernel fusion. I specialize in optimizing high-performance machine learning libraries, focusing on graph optimization and operator fusion for deep learning workloads. My expertise extends to compiler optimization and hand-tuned SIMD kernels, ensuring peak performance for mobile and edge devices. With a strong interest in HW/SW co-design, I thrive at the intersection of software and hardware, enabling efficient AI systems.

Favorite subreddit: r/localllama, where I stay updated on the latest in LLMs and local AI innovation.

